{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34e5ae83",
      "metadata": {
        "id": "34e5ae83"
      },
      "source": [
        "# Assignment 1: Wrangling and EDA\n",
        "### Foundations of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "691fcb3e",
      "metadata": {
        "id": "691fcb3e"
      },
      "source": [
        "**Q1.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `airbnb_NYC.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the Minnesota police use of for data, `mn_police_use_of_force.csv`, clean the `subject_injury` variable, handling the NA's; this gives a value `Yes` when a person was injured by police, and `No` when no injury occurred. What proportion of the values are missing? Cross-tabulate your cleaned `subject_injury` variable with the `force_type` variable. Are there any patterns regarding when the data are missing? For the remaining missing values, replace the `np.nan/None` values with the label `Missing`.\n",
        "3. Dummy variable: For `metabric.csv`, convert the `Overall Survival Status` variable into a dummy/binary variable, taking the value 0 if the patient is deceased and 1 if they are living.\n",
        "4. Missing values: For `airbnb_NYC.csv`, determine how many missing values of `Review Scores Rating` there are. Create a new variable, in which you impute the median score for non-missing observations to the missing ones. Why might this bias or otherwise negatively impact your results?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_data(force=False):\n",
        "    \"\"\"Download and extract course data from Zenodo.\"\"\"\n",
        "\n",
        "    zip_path = 'data.zip'\n",
        "    data_dir = './data'\n",
        "\n",
        "    if not os.path.exists(zip_path) or force:\n",
        "        print(\"Downloading course data...\")\n",
        "        urllib.request.urlretrieve(\n",
        "            'https://zenodo.org/records/18235955/files/data.zip?download=1',\n",
        "            zip_path\n",
        "        )\n",
        "        print(\"Download complete\")\n",
        "\n",
        "    if not os.path.exists(data_dir) or force:\n",
        "        print(\"Extracting data files...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        print(\"Data extracted\")\n",
        "\n",
        "    return data_dir\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_data()"
      ],
      "metadata": {
        "id": "nXi_U9BcmBZU",
        "outputId": "2cde87cf-5288-41da-9149-d1b192739a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nXi_U9BcmBZU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading course data...\n",
            "Download complete\n",
            "Extracting data files...\n",
            "Data extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "airbnb = pd.read_csv(\"airbnb_NYC.csv\")\n"
      ],
      "metadata": {
        "id": "wvY3x-3axTgc",
        "outputId": "d10b35e9-94cc-48a6-9623-d72528769bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "id": "wvY3x-3axTgc",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'airbnb_NYC.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1169295245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mairbnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"airbnb_NYC.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'airbnb_NYC.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "772a66bd",
      "metadata": {
        "id": "772a66bd"
      },
      "source": [
        "**Q2.** Go to https://sharkattackfile.net/ and download their dataset on shark attacks.\n",
        "\n",
        "1. Open the shark attack file using Pandas. It is probably not a csv file, so `read_csv` won't work. What does work?\n",
        "2. Drop any columns that do not contain data.\n",
        "3. What is an observation? Carefully justify your answer, and explain how it affects your choices in cleaning and analyzing the data.\n",
        "4. Clean the year variable. Describe the range of values you see. Filter the rows to focus on attacks since 1940. Are attacks increasing, decreasing, or remaining constant over time?\n",
        "5. Clean the Age variable and make a histogram of the ages of the victims.\n",
        "6. Clean the `Type` variable so it only takes three values: Provoked and Unprovoked and Unknown. What proportion of attacks are unprovoked?\n",
        "7. Clean the `Fatal Y/N` variable so it only takes three values: Y, N, and Unknown.\n",
        "8. Is the attack more or less likely to be fatal when the attack is provoked or unprovoked? Thoughts?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e54a370",
      "metadata": {
        "id": "0e54a370"
      },
      "source": [
        "**Q3.** Open the \"tidy_data.pdf\" document available in `https://github.com/ds4e/wrangling`, which is a paper called *Tidy Data* by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a58495aa",
      "metadata": {
        "id": "a58495aa"
      },
      "source": [
        "**Q4.** This question looks at financial transfers from international actors to American universities. In particular, from which countries and giftors are the gifts coming from, and to which institutions are they going?\n",
        "\n",
        "For this question, `.groupby([vars]).count()` and `.groupby([vars]).sum()` will be especially useful to tally the number of occurrences and sum the values of those occurrences.\n",
        "\n",
        "1. Load the `ForeignGifts_edu.csv` dataset.\n",
        "2. For `Foreign Gift Amount`, create a histogram and describe the variable. Describe your findings.\n",
        "3. For `Gift Type`, create a histogram or value counts table. What proportion of the gifts are contracts, real estate, and monetary gifts?\n",
        "4. What are the top 15 countries in terms of the number of gifts? What are the top 15 countries in terms of the amount given?\n",
        "5. What are the top 15 institutions in terms of the total amount of money they receive? Make a histogram of the total amount received by all institutions.\n",
        "6. Which giftors provide the most money, in total?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "465a8e97",
      "metadata": {
        "id": "465a8e97"
      },
      "source": [
        "**Q5.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "We'll use the `college_completion.csv` dataset from the US Department of Education. The \"relevant\" variables for this question are:\n",
        "  - `level` - Level of institution (4-year, 2-year)\n",
        "  - `aid_value` - The average amount of student aid going to undergraduate recipients\n",
        "  - `control` - Public, Private not-for-profit, Private for-profit\n",
        "  - `grad_100_value` - percentage of first-time, full-time, degree-seeking undergraduates who complete a degree or certificate program within 100 percent of expected time (bachelor's-seeking group at 4-year institutions)\n",
        "\n",
        "1. Load the `college_completion.csv` data with Pandas.\n",
        "2. How many observations and variables are in the data? Use `.head()` to examine the first few rows of data.\n",
        "3. Cross tabulate `control` and `level`. Describe the patterns you see in words.\n",
        "4. For `grad_100_value`, create a kernel density plot and describe table. Now condition on `control`, and produce a kernel density plot and describe tables for each type of institutional control. Which type of institution appear to have the most favorable graduation rates?\n",
        "5. Make a scatterplot of `grad_100_value` by `aid_value`, and compute the covariance and correlation between the two variables. Describe what you see. Now make the same plot and statistics, but conditioning on `control`. Describe what you see. For which kinds of institutions does aid seem to vary positively with graduation rates?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1a468b",
      "metadata": {
        "id": "8b1a468b"
      },
      "source": [
        "**Q6.** In class, we talked about how to compute the sample mean of a variable $X$,\n",
        "$$\n",
        "m(X) = \\dfrac{1}{N} \\sum_{i=1}^N x_i\n",
        "$$\n",
        "and sample covariance of two variables $X$ and $Y$,\n",
        "$$\n",
        "\\text{cov}(X,Y) = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))(y_i - m(Y))).\n",
        "$$\n",
        "Recall, the sample variance of $X$ is\n",
        "$$\n",
        "s^2 = \\dfrac{1}{N} \\sum_{i=1}^N (x_i - m(X))^2.\n",
        "$$\n",
        "It can be very helpful to understand some basic properties of these statistics. If you want to write your calculations on a piece of paper, take a photo, and upload that to your GitHub repo, that's probably easiest.\n",
        "\n",
        "We're going to look at **linear transformations** of $X$, $Y = a + bX$. So we take each value of $X$, $x_i$, and transform it as $y_i = a + b x_i$.\n",
        "\n",
        "1. Show that $m(a + bX) = a+b \\times m(X)$.\n",
        "2. Show that $ \\text{cov}(X,X) = s^2$.\n",
        "3. Show that $\\text{cov}(X,a+bY) = b \\times \\text{cov}(X,Y)$\n",
        "4. Show that $\\text{cov}(a+bX,a+bY) = b^2 \\text{cov}(X,Y) $. Notice, this also means that $\\text{cov}(bX, bX) = b^2 s^2$.\n",
        "5. Suppose $b>0$ and let the median of $X$ be $\\text{med}(X)$. Is it true that the median of $a+bX$ is equal to $a + b \\times \\text{med}(X)$? Is the IQR of $a + bX$ equal to $a + b \\times \\text{IQR}(X)$?\n",
        "6. Show by example that the means of $X^2$ and $\\sqrt{X}$ are generally not $(m(X))^2$ and $\\sqrt{m(X)}$. So, the results we derived above really depend on the linearity of the transformation $Y = a + bX$, and transformations like $Y = X^2$ or $Y = \\sqrt{X}$ will not behave in a similar way."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f54b39",
      "metadata": {
        "id": "a5f54b39"
      },
      "source": [
        "**Q7.** This question provides some practice doing exploratory data analysis and visualization.\n",
        "\n",
        "We'll use the `ames_prices.csv` dataset. The \"relevant\" variables for this question are:\n",
        "  - `price` - Sale price value of the house\n",
        "  - `Bldg.Type` - Building type of the house (single family home, end-of-unit townhome, duplex, interior townhome, two-family conversion)\n",
        "\n",
        "1. Load the `college_completion.csv` data with Pandas.\n",
        "2. Make a kernel density plot of price and compute a describe table. Now, make a kernel density plot of price conditional on building type, and use `.groupby()` to make a describe type for each type of building. Which building types are the most expensive, on average? Which have the highest variance in transaction prices?\n",
        "3. Make an ECDF plot of price, and compute the sample minimum, .25 quantile, median, .75 quantile, and sample maximum (i.e. a 5-number summary).\n",
        "4. Make a boxplot of price. Are there outliers? Make a boxplot of price conditional on building type. What patterns do you see?\n",
        "5. Make a dummy variable indicating that an observation is an outlier.\n",
        "6. Winsorize the price variable, and compute a new kernel density plot and describe table. How do the results change?"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}